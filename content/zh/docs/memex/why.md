---
title: 为什么选择 Memex？
description: 理解开发痛点以及 Memex 的设计初衷
navigation:
  order: 2
---

*或者，更喜欢整活版本？[这边请 🕯️](/docs/memex/meme)*

---

我们每天都在使用 `Claude Code`，同时也用 `Codex`、`Gemini` 和 `OpenCode`。

在开发过程中，我们经常需要找回过去对话中的细节——某个具体的实现细节、特定的参数值，或者仅仅是想确认"上次到底做了什么，以便回滚"。

逐个翻找会话记录非常低效，而且这些工具之间互不通气——在 `Claude` 里讨论过的内容，新的上下文就彻底消失了，更不用说换到另一个工具里了。为了解决这个断层，我们构建了 `Memex`。

## 完整保留，永不压缩

我不想我的信息被压缩而丢失细节。

在找某个历史信息时，细节本身才是最重要的。因此，`Memex` 选择不压缩存储。没有压缩，没有 AI 的二次重写。你搜到的就是最真实的原始对话，而不是一个"大概发生了什么"的版本。

当然，我们也提供了可选的 `compact` 功能，用于优化 AI 调用 `MCP` 时多层检索减少 `Token` 消耗，同时我们计划在这个基础上再做一些好玩的东西出来。

但无论如何，最底层的存储数据永远是你和 `AI CLI` 的对话内容，不会做压缩。

## 跨工具的统一记忆

我们发现通过 `JSONL` 的方式采集对话内容，并通过 `MCP` 的方式提供给 `AI CLI` 后，似乎可以打破不同 `CLI` 之间的信息墙，因为记忆是在 `Memex` 上的。

## 完全定制，丰俭由人

我们认为每个人的工作流都不尽相同。

有人习惯偶尔手动搜索，有人希望自动加载上下文，有人想要压缩摘要，而有人则坚持要原始记录，有的人有云上的服务可以用，有的人更在乎隐私需要本地运行。

所以 `Memex` 支持按需搜索——由你决定在什么时候加载、加载什么，你也可以选择加上下文注入功能，形成永不断链的长期会话。

我们尽可能把所有在采集逻辑之上的功能配置化，核心目标始终是保持灵活。我们把选择权交给你，你可以根据自己的情况来调整使用方式。
